{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2409df4b",
   "metadata": {},
   "source": [
    "Source: https://github.com/abachaa/VQA-Med-2019\n",
    "\n",
    "# VQA-Med-2019\n",
    "\n",
    "- Website: https://www.imageclef.org/2019/medical/vqa/\n",
    "- Mailing list: https://groups.google.com/d/forum/imageclef-vqa-med \n",
    "- Paper: http://ceur-ws.org/Vol-2380/paper_272.pdf\n",
    "- Results of the VQA-Med-2019 challenge on crowdAI: https://www.crowdai.org/challenges/imageclef-2019-vqa-med/leaderboards \n",
    "\n",
    "Task:\n",
    "-------------------\n",
    "VQA-Med 2019 focused on radiology images and four main categories of questions: Modality, Plane, Organ system and Abnormality. These categories are designed with different degrees of difficulty leveraging both classification and text generation approaches. In this second edition of the VQA challenge, we targeted medical questions asking about one element only (e.g. what is the organ principally shown in this mri? in what plane is this mammograph taken? is this a t1 weighted, t2 weighted, or flair image? what is most alarming about this ultrasound?), and that can be answered from the image content without requiring additional medical knowledge or domain-specific inference.  \n",
    "\n",
    "VQA-Med-2019 Data:\n",
    "-------------------\n",
    "The VQA-Med-2019 dataset includes a training set of 3,200 medical images with 12,792 Question-Answer (QA) pairs, a validation set of 500 medical images with 2,000 QA pairs, and a test set of 500 medical images with 500 questions. \n",
    "\n",
    "1) The training set: https://github.com/abachaa/VQA-Med-2019/blob/master/ImageClef-2019-VQA-Med-Training.zip  \n",
    "\n",
    "2) The validation set: https://github.com/abachaa/VQA-Med-2019/blob/master/ImageClef-2019-VQA-Med-Validation.zip\n",
    "\n",
    "3) The VQA-Med-2019 test set and the reference answers are available here: https://github.com/abachaa/VQA-Med-2019/tree/master/VQAMed2019Test  \n",
    "\n",
    "Please see the readme files for more detailed information about the dataset and the categories of questions and answers:\n",
    "https://github.com/abachaa/VQA-Med-2019/blob/master/README-VQA-Med-2019-Data.txt\n",
    "https://github.com/abachaa/VQA-Med-2019/blob/master/VQAMed2019Test/README-VQA-Med-2019-TestSet.txt\n",
    "\n",
    "Reference: \n",
    "-------------------\n",
    "\n",
    "If you use the VQA-Med 2019 dataset, please cite our paper:\n",
    "\"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019\". Asma Ben Abacha, Sadid A. Hasan, Vivek V. Datla, Joey Liu, Dina Demner-Fushman, Henning MÃ¼ller. CLEF 2019 Working Notes.  \n",
    "\n",
    "@Inproceedings{ImageCLEFVQA-Med2019,\n",
    "\n",
    "        author = {Asma {Ben Abacha} and Sadid A. Hasan and Vivek V. Datla and Joey Liu and Dina Demner-Fushman and Henning M\\\"uller},\n",
    "        title = {VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019},\n",
    "        \n",
    "        booktitle = {CLEF 2019 Working Notes},\n",
    "        \n",
    "        series = {{CEUR} Workshop Proceedings},\n",
    "        \n",
    "        year = {2019},\n",
    "        \n",
    "        publisher = {CEUR-WS.org $<$http://ceur-ws.org$>$},\n",
    "        \n",
    "        month = {September 9-12},\n",
    "        \n",
    "        address = {Lugano, Switzerland}\n",
    "        }\n",
    "        \n",
    " Contact Information\n",
    " -------------------\n",
    "Asma Ben Abacha: asma.benabacha@nih.gov   https://sites.google.com/site/asmabenabacha/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image as display_image \n",
    "\n",
    "display_image(filename=os.path.join('references', 'leaderboards.png')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c351c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_gpu sentence-transformers matplotlib nltk nlpaug scikit-learn pandas ipywidgets opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nltk\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "aug = naw.SynonymAug(aug_src='wordnet', stopwords=stopwords)\n",
    "sbert_model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab399c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_val_set = {\n",
    "    'train': {},\n",
    "    'val': {},\n",
    "}\n",
    "\n",
    "df = pd.read_csv('train_val.csv')\n",
    "\n",
    "for key in ['id', 'category', 'question', 'answer', 'part']:\n",
    "    df[key] = df[key].str.lower().str.strip()\n",
    "\n",
    "categories = set(df['category'].unique())\n",
    "cat_map = dict([(cat, idx) for (idx, cat) in enumerate(categories)])\n",
    "cat_map_rev = dict([(idx, cat) for (idx, cat) in enumerate(categories)])\n",
    "\n",
    "# prepare text data\n",
    "\n",
    "def augment_df(\n",
    "    df: pd.DataFrame,\n",
    "    aug_loops: int = 4,\n",
    "    col: str = 'question',\n",
    "    key_skip=None\n",
    ") -> pd.DataFrame:\n",
    "    records = df.to_dict('records')\n",
    "    augmented = []\n",
    "    \n",
    "    count = len(records)\n",
    "    \n",
    "    augment = aug.augment\n",
    "    augmented_add = augmented.append\n",
    "    for (row_idx, row) in enumerate(records):\n",
    "        print(f'\\rAugmenting {row_idx + 1} / {count}', end='')\n",
    "        if key_skip(row):\n",
    "            continue\n",
    "        for _ in range(aug_loops):\n",
    "            new_row = row.copy()\n",
    "            new_row[col] = augment(str(row[col]))\n",
    "            augmented_add(new_row)\n",
    "    print('')\n",
    "    records.extend(augmented)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def create_embedding(arr: List[str], key=None):\n",
    "    if key is None:\n",
    "        res = sbert_model.encode(arr)\n",
    "    else:\n",
    "        res = sbert_model.encode([key(x) for x in arr])\n",
    "    res = res.reshape(len(res), 64, 12)\n",
    "    res = np.array(res, dtype=np.float64)\n",
    "    return res\n",
    "\n",
    "def avg(arr, round_key=lambda x: x):\n",
    "    return round_key(sum(arr) / len(arr))\n",
    "\n",
    "name_labels_map = {}\n",
    "for (row_idx, row) in df.iterrows():\n",
    "    name = row['id']\n",
    "    cat = row['category']\n",
    "    assert(cat in categories), 'unknown category in df'    \n",
    "    if name not in name_labels_map:\n",
    "        name_labels_map[name] = set()\n",
    "    name_labels_map[name].add(cat)\n",
    "\n",
    "\n",
    "def get_part(_df, part: str) -> List[dict]:\n",
    "    res = _df[_df['part'] == part]\n",
    "    res = shuffle(res, random_state=seed)\n",
    "    res = res.to_dict('records')\n",
    "    return res\n",
    "\n",
    "augdf = augment_df(df, key_skip=lambda x: x['part'] == 'val')\n",
    "train = get_part(augdf, 'train')\n",
    "val = get_part(augdf, 'val')\n",
    "\n",
    "print('Encoding data')\n",
    "for part in [train, val]:\n",
    "    for (idx, elem) in enumerate(part):\n",
    "        txt = [elem['question']]\n",
    "        part[idx]['embedding'] = create_embedding(txt)[0]\n",
    "\n",
    "# X_train, y_train, X_val, y_val = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7089353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images into memory\n",
    "image_map = {}\n",
    "image_folder = os.path.join('.', 'images')\n",
    "\n",
    "image_shapes = []\n",
    "average_shape = None\n",
    "\n",
    "# get all image shapes to find the average shape\n",
    "for filename in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    image_shapes.append(cv2.imread(image_path).shape)\n",
    "    \n",
    "x, y, z = zip(*image_shapes)\n",
    "x, y, z = avg(x, round), avg(y, round), avg(z, round)\n",
    "average_shape = (x, y, z)\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    name, _ = os.path.splitext(os.path.basename(image_path))\n",
    "    assert(os.path.isfile(image_path)), 'Missing image'\n",
    "    image_map[name] = cv2.imread(image_path)\n",
    "    image_map[name] = cv2.resize(image_map[name], average_shape[:-1])\n",
    "    image_map[name] = np.array(image_map[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input_shape = average_shape[:-1]\n",
    "txt_input_shape = train[0]['embedding'].shape\n",
    "print(f'{img_input_shape=}; {txt_input_shape=}')\n",
    "img_input_shape+txt_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6596a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imf = image_map[list(image_map.keys())[random.randint(0, len(image_map.keys()))]]\n",
    "plt.imshow(imf, cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64762be",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = keras.applications.resnet50.ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b17887",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_units = 1024\n",
    "num_hidden_layers = 3\n",
    "batch_size = 128\n",
    "dropout = 0.5\n",
    "activation = 'tanh'\n",
    "img_dim = 4096\n",
    "word2vec_dim = 300\n",
    "num_epochs = 100\n",
    "nb_classes = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887271fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Reshape, Concatenate\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_hidden_units, input_dim=word2vec_dim+img_dim,\n",
    "                kernel_initializer='uniform'))\n",
    "model.add(Dropout(dropout))\n",
    "for i in range(num_hidden_layers):\n",
    "    model.add(Dense(num_hidden_units, kernel_initializer='uniform'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(nb_classes, kernel_initializer='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "# tensorboard = TensorBoard(log_dir='./Graph',\n",
    "#                           histogram_freq=0,\n",
    "#                           write_graph=True,\n",
    "#                           write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaea196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba83fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91337a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_hidden_units_LSTM = 512\n",
    "max_length_questions = 30\n",
    "\n",
    "### Image model\n",
    "model_image = Sequential()\n",
    "model_image.add(Reshape((img_dim,), input_shape=(img_dim,)))\n",
    "\n",
    "### Language Model\n",
    "model_language = Sequential()\n",
    "model_language.add(LSTM(number_of_hidden_units_LSTM,\n",
    "                        return_sequences=True,\n",
    "                        input_shape=(max_length_questions,\n",
    "                                    word2vec_dim)))\n",
    "model_language.add(LSTM(number_of_hidden_units_LSTM,\n",
    "                        return_sequences=True))\n",
    "model_language.add(LSTM(number_of_hidden_units_LSTM,\n",
    "                        return_sequences=False))\n",
    "\n",
    "### Merging the two models\n",
    "# full_model = Sequential()\n",
    "# full_model.add(Concatenate([model_language, model_image],\n",
    "#                   mode='concat',\n",
    "#                   concat_axis=1))\n",
    "\n",
    "# full_model = Model([model_language, model_image], model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015f24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
